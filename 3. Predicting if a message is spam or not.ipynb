{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting if a message is spam or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv(r'C:\\Users\\sharm.LAPTOP-118C54MT\\OneDrive - York University\\Coursera\\Course_4\\Assignment_3\\spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage of the spam documents in spam_data\n",
    "(spam_data['target'].sum()/spam_data['target'].count())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#fit training data using a count vectorizer with default parameters and get the longest string\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "longest_string = max(vect.get_feature_names(), key=len)\n",
    "longest_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#fit a mulinomial naive bayes classifier model with smoothing alpha = 0.1. Find AUC score using the \n",
    "#transformed test data \n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "clfrNB =  MultinomialNB(alpha=0.1)\n",
    "clfrNB.fit(X_train_vectorized, y_train)\n",
    "predictions = clfrNB.predict(vect.transform(X_test))\n",
    "roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "0.074475          aaniye\n",
      "0.074475        athletic\n",
      "0.074475            chef\n",
      "0.074475       companion\n",
      "0.074475      courageous\n",
      "0.074475      dependable\n",
      "0.074475      determined\n",
      "0.074475    exterminator\n",
      "0.074475          healer\n",
      "0.074475        listener\n",
      "0.074475       organizer\n",
      "0.074475            pest\n",
      "0.074475    psychiatrist\n",
      "0.074475    psychologist\n",
      "0.074475         pudunga\n",
      "0.074475         stylist\n",
      "0.074475     sympathetic\n",
      "0.074475          venaam\n",
      "0.091250          diwali\n",
      "0.091250        mornings\n",
      "dtype: object index\n",
      "1.000000    146tf150p\n",
      "1.000000          645\n",
      "1.000000     anything\n",
      "1.000000      anytime\n",
      "1.000000      beerage\n",
      "1.000000         done\n",
      "1.000000           er\n",
      "1.000000       havent\n",
      "1.000000         home\n",
      "1.000000          lei\n",
      "1.000000         nite\n",
      "1.000000           ok\n",
      "1.000000         okie\n",
      "1.000000        thank\n",
      "1.000000        thanx\n",
      "1.000000          too\n",
      "1.000000        where\n",
      "1.000000          yup\n",
      "0.980166         tick\n",
      "0.932702        blank\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#fit and transform training data using Tfidf vectorizer with  default parameters.\n",
    "#find 20 features with smallest and largest tf-idf \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer().fit(X_train) #tfidf vectorizer\n",
    "X_train_vectorized = vect.transform(X_train) #fitting on training data \n",
    "feature_names = np.array(vect.get_feature_names()) #get the featuress \n",
    "sorted_tfidf_index = (X_train_vectorized.max(0).toarray()[0]).argsort()\n",
    "smallest_tfidf_feature = feature_names[sorted_tfidf_index[:20]] #smallest tfidf features \n",
    "largest_tfidf_feature = feature_names[sorted_tfidf_index[:-21:-1]]\n",
    "smallest_tfidf_feature_dict = {'value' : smallest_tfidf_feature, 'index' : (X_train_vectorized.max(0).toarray()[0])\n",
    "                                         [sorted_tfidf_index[:20]]}\n",
    "largest_tfidf_feature_dict = {'value' : largest_tfidf_feature, 'index' : (X_train_vectorized.max(0).toarray()[0])\n",
    "                                         [sorted_tfidf_index[:-21:-1]]}\n",
    "smallest_tfidf_feature_df = pd.DataFrame(smallest_tfidf_feature_dict)\n",
    "largest_tfidf_feature_df = pd.DataFrame(largest_tfidf_feature_dict)\n",
    "\n",
    "smallest_tfidf_feature_df.sort_values(by = ['index', 'value'],inplace = True)\n",
    "largest_tfidf_feature_df.sort_values(by = ['index', 'value'],inplace = True, ascending = [False, True])\n",
    "\n",
    "print(pd.Series(smallest_tfidf_feature_df['value'].to_list(), index= smallest_tfidf_feature_df['index']),\\\n",
    "pd.Series(largest_tfidf_feature_df['value'].to_list(), index= largest_tfidf_feature_df['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416243654822335"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC score after fitting data on tfidf vectorizer and then using multinomial Naive Vayes Classifier\n",
    "vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "clfrNB =  MultinomialNB(alpha=0.1)\n",
    "clfrNB.fit(X_train_vectorized, y_train)\n",
    "predictions = clfrNB.predict(vect.transform(X_test))\n",
    "roc_auc_score(y_test,predictions) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.02362694300518 138.8661311914324\n"
     ]
    }
   ],
   "source": [
    "# average length of documents for not spam and spam documents \n",
    "spam = spam_data[spam_data['target']==1]\n",
    "not_spam = spam_data[spam_data['target']!=1]\n",
    "avg_spam = spam['text'].str.len().sum()/len(spam.index)\n",
    "avg_notspam = not_spam['text'].str.len().sum()/len(not_spam.index) \n",
    "print(avg_notspam, avg_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function combines new features into the training data.\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661689557407943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# fit and transform training data using tfidf vectorizer ignoring the document with a frequency lower than 5\n",
    "#combine this document with a new feature the length of document, fit a support vector classification model\n",
    "#with regularization c=1000.\n",
    "#comput the AUC score \n",
    "\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_final = add_feature(X_train_vectorized,X_train.str.len())\n",
    "clfrSVM =  SVC(C=10000)\n",
    "clfrSVM.fit(X_train_final, y_train)\n",
    "predictions = clfrSVM.predict(add_feature(vect.transform(X_test),X_test.str.len()))\n",
    "roc_auc_score(y_test,predictions) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate length of the list for a vectorized string\n",
    "def len_list(x):\n",
    "    ind_len = []\n",
    "    for i in x:\n",
    "        ind_len.append(len(i))\n",
    "    return sum(ind_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2992746113989637 15.759036144578314\n"
     ]
    }
   ],
   "source": [
    "#average number of digits per document for not spam and spam documents\n",
    "spam = spam_data[spam_data['target']==1]\n",
    "not_spam = spam_data[spam_data['target']!=1]\n",
    "spam_digit = spam['text'].str.findall(r'([0-9]+)').apply(len_list)\n",
    "notspam_digit = not_spam['text'].str.findall(r'([0-9]+)').apply(len_list)\n",
    "avg_spam = spam_digit.sum()/len(spam.index)\n",
    "avg_notspam = notspam_digit.sum()/len(not_spam.index)\n",
    "print(avg_notspam, avg_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm.LAPTOP-118C54MT\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9809793219360643"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#fit and transform training data using Tfidf vectorizer ignoring documents with a frequency lower than 5\n",
    "#add features the length of document and number of digits per document to the matrix \n",
    "#fit a logistic regression model with C=100 and calculate AUC.\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_final = add_feature(X_train_vectorized,[X_train.str.len(),\\\n",
    "                                                X_train.str.findall(r'([0-9]+)').apply(len_list)])\n",
    "clfrlreg =  LogisticRegression(C=100)\n",
    "clfrlreg.fit(X_train_final, y_train)\n",
    "predictions = clfrlreg.predict(add_feature(vect.transform(X_test),\\\n",
    "                                          [X_test.str.len(), X_test.str.findall(r'([0-9]+)').apply(len_list)]))\n",
    "\n",
    "roc_auc_score(y_test,predictions) \n",
    "#much better accuracy than before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.29181347150259 29.041499330655956\n"
     ]
    }
   ],
   "source": [
    "#average number of non-word characters per document for not spam and spam documents \n",
    "spam = spam_data[spam_data['target']==1]\n",
    "not_spam = spam_data[spam_data['target']!=1]\n",
    "spam_char = spam['text'].str.findall(r'(\\W+)').apply(len_list)\n",
    "notspam_char = not_spam['text'].str.findall(r'(\\W+)').apply(len_list)\n",
    "avg_spam = spam_char.sum()/len(spam.index)\n",
    "avg_notspam = notspam_char.sum()/len(not_spam.index)\n",
    "    \n",
    "print(avg_notspam, avg_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm.LAPTOP-118C54MT\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733651087380949 [-3.47119919721989, -3.071198885849946, -3.0329634588744185, -2.794473948646245, -2.738412966772945, -2.6916916635565378, -2.6697308889533957, -2.5150605475477223, -2.45198471401025, -2.4501798093530587] [5.399780531301951, 4.724937924225051, 4.6293773893619425, 4.016249722603797, 3.937401048395025, 3.7942602042979883, 3.5290774094342408, 3.5118392561557057, 3.4282033484005368, 3.4005547897044752]\n"
     ]
    }
   ],
   "source": [
    "#fit and transform training data using a count vectorizer ignoring documents that have frequency lower than\n",
    "#5 and using characer n-grams from n=2 to n=5 \n",
    "#add features length of document, number of digits per document and number of non-word characters\n",
    "#fit a logistic regression model with regularization C=100, compute AUC score \n",
    "#find 10 smallest and 10 largest coefficients from the model \n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(2,5), analyzer='char_wb').fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_final = add_feature(X_train_vectorized,[X_train.str.len(),\\\n",
    "                                                X_train.str.findall(r'([0-9]+)').apply(len_list),\\\n",
    "                           X_train.str.findall(r'(\\W+)').apply(len_list)])\n",
    "clfrlreg =  LogisticRegression(C=100)\n",
    "clfrlreg.fit(X_train_final, y_train)\n",
    "predictions = clfrlreg.predict(add_feature(vect.transform(X_test),\\\n",
    "                                          [X_test.str.len(), X_test.str.findall(r'([0-9]+)').apply(len_list),\\\n",
    "                                          X_test.str.findall(r'(\\W+)').apply(len_list)]))\n",
    "AUC = roc_auc_score(y_test,predictions)\n",
    "smallest = list(np.sort(clfrlreg.coef_[0])[:10])\n",
    "largest = list(np.sort(clfrlreg.coef_[0])[:-11:-1])\n",
    "print(AUC,smallest,largest)\n",
    "#great the accuray seems really good after adding features and tweeking some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
